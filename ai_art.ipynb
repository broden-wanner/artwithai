{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Art with Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, start out with some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from keras import backend\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input\n",
    "from scipy.optimize import f_min_l_bfgs_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the paths for the content image, the style image, and the output image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_image_path = ''\n",
    "s_image_path = ''\n",
    "o_image_path = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_height = 512\n",
    "target_width = 512\n",
    "target_size = (target_height, target_width)\n",
    "\n",
    "# Get size of the original image\n",
    "c_image_original = Image.open(content_image_path)\n",
    "c_image_original_size = c_image_original.size\n",
    "# Loads content image to PIL format\n",
    "c_image = load_img(path=content_image_path, target_size=target_size)\n",
    "# Turns image into extended numpy array\n",
    "c_image_arr = img_to_array(c_image)\n",
    "# Expand the first dimension, preprocess input for given model, and add array to the backend\n",
    "# (The extra dimension is added because keras expects an array of samples; thus, the image must be\n",
    "# part of an array)\n",
    "c_image_arr = backend.variable(preprocess_input(np.expand_dims(c_image_arr, axis=0)), dtype='float32', name='c_image_arr')\n",
    "\n",
    "# Perform same operations on style image...\n",
    "s_image = load_img(path=s_image_path, target_size=target_size)\n",
    "s_image_arr = img_to_array(s_image)\n",
    "s_image_arr = backend.variable(preprocess_input(np.expend_dims(s_image_arr, axis=0)), dtype='float32')\n",
    "\n",
    "# Initialize the output image as numpy array of shape (target_width, target_height, 3) with random RGB values\n",
    "o_image_initial = np.random.randint(256, size=(target_width, target_height, 3)).astype('float64')\n",
    "o_image_initial = preprocess_input(np.expand(dims, axis=0))\n",
    "# Instantiates a placeholder tensor\n",
    "o_image_placeholder = backend.placeholder(shape=(1, target_width, target_height, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Loss\n",
    "The content loss function must be formulated in order to ensure that the generated image x retains some of the \"global\" characteristics of the content image p. To achieve this, the content loss function is defined as the mean squared error between the feature representations of p and x, respectively, at a given layer l:\n",
    "\n",
    "$$ L_c(p,x,l) = \\frac{1}{2}\\sum_{i,j}^{} (F^l_{i,j} - P^l_{i,j})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><i>F</i> and <i>P</i> and are matrices of size <i>N</i> x <i>M</i></li>\n",
    "    <li><i>N</i> is the number of filters in layer <i>l</i> and <i>M</i> is the number of spatial elements in the feature map (height times width) for layer <i>l</i></li>\n",
    "    <li><i>F</i> contains the feature representation of <i>x</i> for layer <i>l</i></li>\n",
    "    <li><i>P</i> contains the feature representation of <i>p</i> for layer <i>l</i></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_reps(x, layer_names, model):\n",
    "    \"\"\"\n",
    "    Get feature representations of input x for one or more layers in a given model.\n",
    "    \"\"\"\n",
    "    f_matrices = []\n",
    "    for l in layer_names:\n",
    "        current_layer = model.get_layer(l)\n",
    "        feature_raw = current_layer.output\n",
    "        feature_raw_shape = backend.shape(feature_raw).eval(session=tf_session)\n",
    "        N_l = feature_raw_shape[-1]\n",
    "        M_l = feature_raw_shape[1] * feature_raw_shape[2]\n",
    "        feature_matrix = backend.reshape(feature_raw, (M_l, N_l))\n",
    "        feature_matrix = backend.transpose(feature_matrix)\n",
    "        feature_matrices.append(feature_matrix)\n",
    "    return feature_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_loss(F, P):\n",
    "    content_loss = 1/2 * backend.sum(backend.square(F - P))\n",
    "    return content_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Loss\n",
    "Conversely, style loss is designed to preserve the stylisitc characteristics of the style image, <i>a</i>. As opposed to using the difference between feature representations, use the difference between Gram matrices from selected layers. The Gram matrix is a square matrix that contains the dot products between each vectorized filter in layer <i>l</i>, and it can therefore be thought of as a non-normalized correlation matrix for filters in the layer. The Gram matrix is defined as follows:\n",
    "    \n",
    "$$ G^l = F^l(F^l)^T $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gram_matrix(F):\n",
    "    G = backend.dot(F, K.transpose(F))\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the loss contribution from layer <i>l</i> is\n",
    "\n",
    "$$ E_l = \\frac{1}{4N^2_lM^2_l} \\sum_{i,j}(G^l_{ij}-A^l_{ij})^2$$\n",
    "\n",
    "where <i>A</i> is the Gram matrix for the style image <i>a</i> and <i>G</i> is the Gram matrix for the generate image <i>x</i>. Ascending layers in most convolutional networks such as VGG have increasingly larger receptive fields. As this receptive field grows, more large-scale characteristics of the input image are preserved. Because of this, multiple layers should be selected for “style” to incorporate both local and global stylistic qualities. To create a smooth blending between these different layers, we can assign a weight <i>w</i> to each layer, and define the total style loss as:\n",
    "\n",
    "$$ L_s(a,x,l) = \\sum_l^L w_lE_l $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_loss(ws, Gs, As):\n",
    "    style_loss = backend.variable(0.)\n",
    "    for w, G, A in zip(ws, Gs, As):\n",
    "        M_l = backend.int_shape(G)[1]\n",
    "        N_l = backend.int_shape(G)[0]\n",
    "        G_gram = get_gram_matrix(G)\n",
    "        A_gram = get_gram_matrix(A)\n",
    "        style_loss += w * 1/4 * backend.sum(backend.square(G_gram - A_gram)) / (N_l**2 * M_l**2)\n",
    "    return style_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
