{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Art with Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, start out with some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from keras import backend\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input\n",
    "from scipy.optimize import fmin_l_bfgs_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the paths for the content image, the style image, and the output image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_image_path      = './initial_images/content_image.jpg'\n",
    "s_image_path      = './initial_images/style_image.jpg'\n",
    "o_image_directory = './results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_height = 512\n",
    "target_width = 512\n",
    "target_size = (target_height, target_width)\n",
    "\n",
    "# Get size of the original image\n",
    "c_image_original = Image.open(c_image_path)\n",
    "c_image_original_size = c_image_original.size\n",
    "# Loads content image to PIL format\n",
    "c_image = load_img(path=c_image_path, target_size=target_size)\n",
    "# Turns image into extended numpy array\n",
    "c_image_arr = img_to_array(c_image)\n",
    "# Expand the first dimension, preprocess input for given model, and add array to the backend\n",
    "# (The extra dimension is added because keras expects an array of samples; thus, the image must be\n",
    "# part of an array)\n",
    "c_image_arr = backend.variable(preprocess_input(np.expand_dims(c_image_arr, axis=0)), dtype='float32', name='c_image_arr')\n",
    "\n",
    "# Perform same operations on style image...\n",
    "s_image = load_img(path=s_image_path, target_size=target_size)\n",
    "s_image_arr = img_to_array(s_image)\n",
    "s_image_arr = backend.variable(preprocess_input(np.expand_dims(s_image_arr, axis=0)), dtype='float32')\n",
    "\n",
    "# Initialize the output image as numpy array of shape (target_width, target_height, 3) with random RGB values\n",
    "o_image_initial = np.random.randint(256, size=(target_width, target_height, 3)).astype('float64')\n",
    "o_image_initial = preprocess_input(np.expand_dims(o_image_initial, axis=0))\n",
    "# Instantiates a placeholder tensor\n",
    "o_image_placeholder = backend.placeholder(shape=(1, target_width, target_height, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Loss\n",
    "The content loss function must be formulated in order to ensure that the generated image x retains some of the \"global\" characteristics of the content image p. To achieve this, the content loss function is defined as the mean squared error between the feature representations of p and x, respectively, at a given layer l:\n",
    "\n",
    "$$ L_c(p,x,l) = \\frac{1}{2}\\sum_{i,j}^{} (F^l_{i,j} - P^l_{i,j})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><i>F</i> and <i>P</i> and are matrices of size <i>N</i> x <i>M</i></li>\n",
    "    <li><i>N</i> is the number of filters in layer <i>l</i> and <i>M</i> is the number of spatial elements in the feature map (height times width) for layer <i>l</i></li>\n",
    "    <li><i>F</i> contains the feature representation of <i>x</i> for layer <i>l</i></li>\n",
    "    <li><i>P</i> contains the feature representation of <i>p</i> for layer <i>l</i></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_reps(x, layer_names, model):\n",
    "    \"\"\"\n",
    "    Get feature representations of input x for one or more layers in a given model.\n",
    "    \"\"\"\n",
    "    f_matrices = []\n",
    "    for l in layer_names:\n",
    "        current_layer = model.get_layer(l)\n",
    "        feature_raw = current_layer.output\n",
    "        feature_raw_shape = backend.shape(feature_raw).eval(session=tf_session)\n",
    "        N_l = feature_raw_shape[-1]\n",
    "        M_l = feature_raw_shape[1] * feature_raw_shape[2]\n",
    "        feature_matrix = backend.reshape(feature_raw, (M_l, N_l))\n",
    "        feature_matrix = backend.transpose(feature_matrix)\n",
    "        feature_matrices.append(feature_matrix)\n",
    "    return feature_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_loss(F, P):\n",
    "    content_loss = 1/2 * backend.sum(backend.square(F - P))\n",
    "    return content_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Loss\n",
    "Conversely, style loss is designed to preserve the stylisitc characteristics of the style image, <i>a</i>. As opposed to using the difference between feature representations, use the difference between Gram matrices from selected layers. The Gram matrix is a square matrix that contains the dot products between each vectorized filter in layer <i>l</i>, and it can therefore be thought of as a non-normalized correlation matrix for filters in the layer. The Gram matrix is defined as follows:\n",
    "    \n",
    "$$ G^l = F^l(F^l)^T $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gram_matrix(F):\n",
    "    G = backend.dot(F, K.transpose(F))\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the style loss contribution from layer <i>l</i> is\n",
    "\n",
    "$$ E_l = \\frac{1}{4N^2_lM^2_l} \\sum_{i,j}(G^l_{ij}-A^l_{ij})^2$$\n",
    "\n",
    "where <i>A</i> is the Gram matrix for the style image <i>a</i> and <i>G</i> is the Gram matrix for the generated image <i>x</i>. Ascending layers in most convolutional networks such as VGG have increasingly larger receptive fields. As this receptive field grows, more large-scale characteristics of the input image are preserved. Because of this, multiple layers should be selected for “style” to incorporate both local and global stylistic qualities. To create a smooth blending between these different layers, we can assign a weight <i>w</i> to each layer, and define the total style loss as:\n",
    "\n",
    "$$ L_s(a,x,l) = \\sum_l^L w_lE_l $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_loss(ws, Gs, As):\n",
    "    style_loss = backend.variable(0.)\n",
    "    for w, G, A in zip(ws, Gs, As):\n",
    "        M_l = backend.int_shape(G)[1]\n",
    "        N_l = backend.int_shape(G)[0]\n",
    "        G_gram = get_gram_matrix(G)\n",
    "        A_gram = get_gram_matrix(A)\n",
    "        style_loss += w * 1/4 * backend.sum(backend.square(G_gram - A_gram)) / (N_l**2 * M_l**2)\n",
    "    return style_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the Losses\n",
    "Lastly, weighting coefficients need to be assigned to both the content and style loss functions and summed to get an overall loss function.\n",
    "\n",
    "$$ L(p, a, x, l) = \\alpha L_c(p,x,l) + \\beta L_s(a, x, l) $$\n",
    "\n",
    "$\\alpha$ and $\\beta$ are hyperparameters that weight the conent and style loss, respectively, and they can be used to tune the influence of both images on the generated image. The authors of the paper recomend setting $\\alpha = 1$ and $\\beta = 10,000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_loss(o_image_placeholder, alpha=1.0, beta=10000.0):\n",
    "    F = get_feature_reps(o_image_placeholder, layer_names=[c_layer_name], model=o_model)[0]\n",
    "    Gs = get_feature_reps(o_image_placeholder, layer_names=s_layer_names, model=g_model)\n",
    "    content_loss = get_content_loss(F, P)\n",
    "    style_loss = get_style_loss(ws, Gs, As)\n",
    "    total_loss = alpha * content_loss + beta * style_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "To start changing the generated image to minimize the loss function, two more functions must be defined to use scipy and the keras backend:\n",
    "\n",
    "1. A function to calculate the total loss.\n",
    "2. A function to calculate the gradient.\n",
    "\n",
    "Both functions are fed as input into a scipy optimization function as the objective and gradient functions respectively. In this case, the limited-memory BFGS algorithm is used. It is an optimization algorithm that approximates the BFGS algorithm using a limited amount of memory, and its goal is to minimize a differentiable, scalar function over unconstrained values of its input vector. For each of the content and style images, the feature representations are extracted to construct <i>P</i> and <i>A</i> (for each selected style layer), and weight the style layers uniformly. In practice, using more than 500 iterations of L-BFGS-B creates convincing visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(o_image_arr):\n",
    "    '''\n",
    "    Calculate total loss using backend.function\n",
    "    '''\n",
    "    if o_image_arr.shape != (1, target_width, target_height, 3):\n",
    "        o_image_arr = o_image_arr.reshape((1, target_width, target_height, 3))\n",
    "    loss_function = backend.function([o_model.input], [get_total_loss(o_model.input)])\n",
    "    return loss_function([o_image_arr])[0].astype('float64')\n",
    "\n",
    "def get_gradient(o_image_arr):\n",
    "    '''\n",
    "    Calculuate the gradient of the loss function with respect to the generated image\n",
    "    '''\n",
    "    if o_image_arr.shape != (1, target_width,  target_height, 3):\n",
    "        o_iamge_arr = o_image_arr.reshape((1, target_width, target_height, 3))\n",
    "        \n",
    "    gradient_function = backend.function([o_model.input], backend.gradients(get_total_loss(o_model.input), [o_model.input]))\n",
    "    gradient = gradient_function([o_image_arr])[0].flatten().astype('float64')\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions must also be defined that process the result of minimizing the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_array(x):\n",
    "    # Zero-center by mean pixel\n",
    "    if x.shape != (target_width, target_height, 3):\n",
    "        x = x.reshape((target_width, target_height, 3))\n",
    "    x[..., 0] += 103.939\n",
    "    x[..., 1] += 116.779\n",
    "    x[..., 2] += 123.68\n",
    "    # 'BGR' -> 'RGB'\n",
    "    x = x[..., ::-1]\n",
    "    x = np.clip(x, 0, 255)\n",
    "    x = x.astype('uint8')\n",
    "    return x\n",
    "\n",
    "def save_image(x, image_number, target_size=c_image_original_size):\n",
    "    x_image = Image.fromarray(x)\n",
    "    x_image = x_image.resize(target_size)\n",
    "    x_image.save(o_image_directory + f'/image_{image_number}.jpg')\n",
    "    return x_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the VGG16 models are initialized with the respective image tensors created earlier. Instead of setting `weights` to `None` (which randomly initializes the weights), `weights` is set to `'imagenet'` (which includes the pre-trained weights on ImageNet). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_session = backend.get_session()\n",
    "c_model = VGG16(include_top=False, weights='imagenet', input_tensor=c_image_arr)\n",
    "s_model = VGG16(include_top=False, weights='imagenet', input_tensor=s_image_arr)\n",
    "o_model = VGG16(include_top=False, weights='imagenet', input_tensor=o_image_placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the layers used to get the feature repesentations are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_layer_name = 'block4_conv2'\n",
    "s_layer_names = [\n",
    "    'block1_conv1',\n",
    "    'block2_conv1',\n",
    "    'block3_conv1',\n",
    "    'block4_conv1',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = get_feature_reps(x=c_image_arr, layer_names=[c_layer_name], model=c_model)\n",
    "As = get_feature_reps(x=s_image_arr, layer_names=s_layer_names, model=s_model)\n",
    "ws = np.ones(len(s_layer_names))/float(len(s_layer_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the image is created using the L-BFGS algorithm from scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 500\n",
    "iterations_per_image = 50\n",
    "x_val = o_image_initial.flatten()\n",
    "\n",
    "for i in range(iterations/iterations_per_image):\n",
    "    start = time.time()\n",
    "    x_val, f_val, info = fmin_l_bfgs_b(\n",
    "                                func    = calculate_loss,\n",
    "                                x0      = x_val,\n",
    "                                fprime  = get_gradient,\n",
    "                                maxiter = iterations_per_image,\n",
    "                                disp    = True\n",
    "                            )\n",
    "    x_output = postprocess_array(x_val)\n",
    "    x_image = save_image(x_output, image_number=i)\n",
    "    print(f'Image {i} Saved')\n",
    "    end = time.time()\n",
    "    print(f'Time taken for image {i}: {end - start}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
